split <- createDataPartition(y=datiFinali$ris, p = 0.7, list = FALSE)
train <- datiFinali[split,]
test <- datiFinali[-split,]
train$ris<-as.factor((train$ris))
train$S1<-as.factor((train$S1))
levels(train$ris) <- c("Win", "Bank")
train<-train[!train$S1=="cesena" | !train$S2=="cesena",]
glm_lasso=train(ris~log(X3+1)+S1*anno+S2*anno+anno+Val_diff, method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
glm_lasso=train(ris~log(X3+1)+S1*anno+S2*anno+anno+valore_rosa.x+valore_rosa.y, method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
datiFinali<-datiP2[,c(1:10,12:15,20:24,28)]
#### Preparazione dati ------
datiFinali$S1<-as.factor(datiFinali$S1)
datiFinali$S2<-as.factor(datiFinali$S2)
datiFinali<-na.omit(datiFinali)
split <- createDataPartition(y=datiFinali$ris, p = 0.7, list = FALSE)
train <- datiFinali[split,]
test <- datiFinali[-split,]
train$ris<-as.factor((train$ris))
train$S1<-as.factor((train$S1))
levels(train$ris) <- c("Win", "Bank")
train<-train[!train$S1=="cesena" | !train$S2=="cesena",]
####  Lasso -----
set.seed(1234)
grid = expand.grid(.alpha=1,.lambda=seq(0, 0.152, by = 0.001)) #non serve farlo arrivare a 1
Control=trainControl(method= "cv",number=10, classProbs=TRUE)
glm_lasso=train(ris~log(X3+1)+S1*anno+S2*anno+anno+valore_rosa.x+valore_rosa.y, method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
glm_lasso=train(ris~S1*anno+S2*anno+anno+valore_rosa.x+valore_rosa.y, method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
gam1<-gam(ris~s(valore_rosa.x)+s(valore_rosa.y),data=datiP2)
summary(gam1)
plot(gam1)
glm_lasso=train(ris~S1*anno+S2*anno+anno+I(valore_rosa.x^3)+log(valore_rosa.y+1), method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
glm_lasso=train(ris~S1*anno+S2*anno+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
glm_lasso=train(ris~S1*anno+S2*anno+anno+poly(valore_rosa.x,4)+log(valore_rosa.y+1), method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
test$p1 = predict(glm_lasso       , test, "prob")[,1]
# roc values
library(pROC)
r1=roc(ris ~ p1, data = test)
plot(r1) #lasso nero
plot(rc,add=T, col="red") #dcontrol
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p1,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.55,0.8,0.025))) #suggerisce un 0.485
glm_lasso=train(ris~S1+S2+anno+poly(valore_rosa.x,4)+log(valore_rosa.y+1), method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
glm_lasso=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), method = "glmnet", data=train,
trControl = Control, tuneLength=10, tuneGrid=grid, metric="Accuracy", na.action=na.exclude)
glm_lasso
plot(glm_lasso)
getTrainPerf(glm_lasso)
plot(varImp(object=glm_lasso),main="train tuned - Variable Importance")
rpartTuneCvA <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "rpart",
tuneLength = 10, na.action=na.exclude, tuneGrid=Grid2,
trControl = cvCtrl, metric="Accuracy")
rpartTuneCvA
getTrainPerf(rpartTuneCvA)
plot(varImp(object=rpartTuneCvA),main="train tuned - Variable Importance")
rfTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "rf",
tuneLength = 2, #troppo lungho con 5
trControl = cvCtrl, na.action=na.exclude, tuneGrid=Grid4) # con tuneLength=5 ci mette tanto
rfTune
getTrainPerf(rfTune)
plot(varImp(object=rfTune),main="train tuned - Variable Importance")
NNTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "nnet",
tuneLength = 4, tuneGrid=Grid0,
trControl = cvCtrl, preProcess="range", na.action=na.exclude) # Non andare oltre il 6 di tuneLength
NNTune
getTrainPerf(NNTune)
XGBTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "xgbTree",
tuneLength = 4,
trControl = cvCtrl, metric="Accuracy", na.action=na.exclude) # Non andare oltre il 6 di tuneLength
XGBTune
getTrainPerf(XGBTune)
NBtune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "nb",
tuneLength = 5, na.action=na.exclude,
trControl = cvCtrl, metric="Sens")
NBtune
getTrainPerf(NBtune)
Deep=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "deepboost",
trControl = cvCtrl, tuneLength=3, na.action=na.exclude)
Deep
getTrainPerf(Deep)
### Risultati ----
results <- resamples(list(Lasso=glm_lasso, Tree=rpartTuneCvA, RandomForest=rfTune,
NeuralNet=NNTune, XGBoosting=XGBTune, NaivaBayes=NBtune, Ada=AdaBTune,
deep=Deep, kknn=kknnT, svm=svmT));results
bwplot(results)
test$p1 = predict(glm_lasso       , test, "prob")[,1]
test$p2 = predict(rpartTuneCvA, test, "prob")[,1]
test$p3 = predict(rfTune    , test, "prob")[,1]
test$p4 = predict(NNTune     , test, "prob")[,1]
test$p5 = predict(XGBTune, test, "prob")[,1]
test$p6 = predict(AdaBTune, test, "prob")[,1]
test$p7 = predict(pls, test, "prob")[,1]
test$p8 = predict(NBtune, test, "prob")[,1]
test$p9 = predict(Deep, test, "prob")[,1]
### Test -----
# estimate probs P(M)
test<-test[!test$S1=="cesena" & !test$S2=="cesena",]
test<-test[!test$S1=="torino"& !test$S2=="torino",]
test<-test[!test$S1=="reggina"& !test$S2=="reggina",]
test<-na.omit(test)
test$p1 = predict(glm_lasso       , test, "prob")[,1]
test$p2 = predict(rpartTuneCvA, test, "prob")[,1]
test$p3 = predict(rfTune    , test, "prob")[,1]
test$p4 = predict(NNTune     , test, "prob")[,1]
test$p5 = predict(XGBTune, test, "prob")[,1]
test$p6 = predict(AdaBTune, test, "prob")[,1]
test$p7 = predict(pls, test, "prob")[,1]
test$p8 = predict(NBtune, test, "prob")[,1]
test$p9 = predict(Deep, test, "prob")[,1]
test$p10 = predict(svmT, test, "prob")[,1]
test$pc<-ifelse(test$X3<2,1,2)
# roc values
library(pROC)
r1=roc(ris ~ p1, data = test)
r2=roc(ris ~ p2, data = test)
r3=roc(ris ~ p3, data = test)
r4=roc(ris ~ p4, data = test)
r5=roc(ris ~ p5, data = test)
r6=roc(ris ~ p6, data = test)
r7=roc(ris ~ p7, data = test)
r8=roc(ris ~ p8, data = test)
r9=roc(ris ~ p9, data = test)
rc=roc(ris ~ pc, data = test)
plot(r1) #lasso nero
plot(r2,add=T,col="red") #Tree
plot(r5,add=T,col="violet") #XGB
plot(r3,add=T,col="blue") #RF
plot(r4,add=T,col="yellow") #NN
plot(r6,add=T,col="orange") #Ada
plot(r7,add=T,col="green") #PLS
plot(r8,add=T) #NB
plot(r9,add=T) #deep
plot(rc,add=T, col="red") #dcontrol
legend("bottomright", c("lasso","Tree", "Random Forest", "NN","XGB","ADA", "PLS", "NB", "Deep"),
text.col=c("black","red","blue","yellow","violet","orange","Green"))
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p5,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.55,0.8,0.025))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.50,0.8,0.025))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.50,0.8,0.01))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.55,0.8,0.01))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.55,0.7,0.01))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.55,0.7,0.05))) #suggerisce un 0.485
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p1,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.55,0.7,0.05))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.50,0.7,0.05))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.487,0.7,0.05))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.50,0.7,0.05))) #suggerisce un 0.485
### Risultati miglior modello, Lasso ------
test$labelsp1<-as.factor(ifelse(test$p1>0.480,1,2))
test$ris<-as.factor(test$ris)
cm<-confusionMatrix(test$labelsp1, test$ris, positive="1") #Tante metriche, bellissimo
draw_confusion_matrix <- function(cm) {
total <- sum(cm$table)
res <- as.numeric(cm$table)
# Generate color gradients. Palettes come from RColorBrewer.
greenPalette <- c("#F7FCF5","#E5F5E0","#C7E9C0","#A1D99B","#74C476","#41AB5D","#238B45","#006D2C","#00441B")
redPalette <- c("#FFF5F0","#FEE0D2","#FCBBA1","#FC9272","#FB6A4A","#EF3B2C","#CB181D","#A50F15","#67000D")
getColor <- function (greenOrRed = "green", amount = 0) {
if (amount == 0)
return("#FFFFFF")
palette <- greenPalette
if (greenOrRed == "red")
palette <- redPalette
colorRampPalette(palette)(100)[10 + ceiling(90 * amount / total)]
}
# set the basic layout
layout(matrix(c(1,1,2)))
par(mar=c(2,2,2,2))
plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
title('CONFUSION MATRIX', cex.main=2)
# create the matrix
classes = colnames(cm$table)
rect(150, 430, 240, 370, col=getColor("green", res[1]))
text(195, 435, classes[1], cex=1.2)
rect(250, 430, 340, 370, col=getColor("red", res[3]))
text(295, 435, classes[2], cex=1.2)
text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
text(245, 450, 'Actual', cex=1.3, font=2)
rect(150, 305, 240, 365, col=getColor("red", res[2]))
rect(250, 305, 340, 365, col=getColor("green", res[4]))
text(140, 400, classes[1], cex=1.2, srt=90)
text(140, 335, classes[2], cex=1.2, srt=90)
# add in the cm results
text(195, 400, res[1], cex=1.6, font=2, col='white')
text(195, 335, res[2], cex=1.6, font=2, col='white')
text(295, 400, res[3], cex=1.6, font=2, col='white')
text(295, 335, res[4], cex=1.6, font=2, col='white')
# add in the specifics
plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
# add in the accuracy information
text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}
draw_confusion_matrix(cm)
### Redditività: dipende dalla quota bank scelta e dalla probabilità di abbinare la scommessa bank ----
tabellaROI<-data.frame()
for(i in seq(1.3, 1.8, by = 0.025)) #la sequenza serve a vedere il comportamento del ROI in funzione della quota
{
test$bet<-ifelse(test$labelsp1==1,10,((10*i)-10))
test$bet_res<-ifelse(test$ris ==test$labelsp1 & test$ris==1, test$bet*test$X3,0)
test$bet_res2<-ifelse(test$ris ==test$labelsp1 & test$ris==2,
10,0)
sum(test$bet_res)
sum(test$bet_res2)
sum(test$bet)
test$ROI<-((test$bet_res)+(test$bet_res2)-(test$bet))/(test$bet)
plot(test$ROI)
rev<-sum(test$bet_res)+sum(test$bet_res2)-sum(test$bet);rev
ROIf<-((sum(test$bet_res)+sum(test$bet_res2)-sum(test$bet))/sum(test$bet))*100 #alto
annualized_ROI<-ROIf/((nrow(test)/365)); annualized_ROI
ROI[i]<-annualized_ROI
test$capital<-(cumsum(test$bet_res)+cumsum(test$bet_res2)-cumsum(test$bet))
plot(test$capital)
print(cbind(ROI[i],i))
x <- c(annualized_ROI,i)
tabellaROI<-rbind(tabellaROI,x)
colnames(tabellaROI)<-c("ROI","Quota_bank")
}
### Risultati ----
results <- resamples(list(Lasso=glm_lasso, Tree=rpartTuneCvA, RandomForest=rfTune,
NeuralNet=NNTune, XGBoosting=XGBTune, NaivaBayes=NBtune, Ada=AdaBTune,
deep=Deep, kknn=kknnT, svm=svmT));results
bwplot(results)
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p3,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.50,0.7,0.05))) #suggerisce un 0.485
plot(r1) #lasso nero
plot(r2,add=T,col="red") #Tree
plot(r3,add=T,col="blue") #RF
plot(r4,add=T,col="yellow") #NN
plot(r5,add=T,col="violet") #XGB
plot(r6,add=T,col="orange") #Ada
plot(r7,add=T,col="green") #PLS
plot(r8,add=T) #NB
plot(r9,add=T) #deep
plot(rc,add=T, col="red") #dcontrol
legend("bottomright", c("lasso","Tree", "Random Forest", "NN","XGB","ADA", "PLS", "NB", "Deep"),
text.col=c("black","red","blue","yellow","violet","orange","Green"))
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p8,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.50,0.7,0.05))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.60,0.7,0.05))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.60,0.7,0.025))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.40,0.7,0.05))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.40,0.7,0.04))) #suggerisce un 0.485
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p5,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.40,0.7,0.04))) #suggerisce un 0.485
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p1,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.40,0.7,0.04))) #suggerisce un 0.485
### Scelta soglia migliore modello -----
pROC_obj <- roc(test$ris,test$p5,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.40,0.7,0.04))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.30,0.7,0.04))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.20,0.7,0.04))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.04))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.05))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.06))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.07))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.02))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.025))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.027))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.028))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.0272))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.02))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.1))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.15))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.151))) #suggerisce un 0.485
plot(pROC_obj, print.thres = quantile(pROC_obj$thresholds, seq(0.10,0.7,0.153))) #suggerisce un 0.485
### Risultati miglior modello, Lasso ------
test$labelsp1<-as.factor(ifelse(test$p5>0.494,1,2))
test$ris<-as.factor(test$ris)
cm<-confusionMatrix(test$labelsp1, test$ris, positive="1") #Tante metriche, bellissimo
draw_confusion_matrix <- function(cm) {
total <- sum(cm$table)
res <- as.numeric(cm$table)
# Generate color gradients. Palettes come from RColorBrewer.
greenPalette <- c("#F7FCF5","#E5F5E0","#C7E9C0","#A1D99B","#74C476","#41AB5D","#238B45","#006D2C","#00441B")
redPalette <- c("#FFF5F0","#FEE0D2","#FCBBA1","#FC9272","#FB6A4A","#EF3B2C","#CB181D","#A50F15","#67000D")
getColor <- function (greenOrRed = "green", amount = 0) {
if (amount == 0)
return("#FFFFFF")
palette <- greenPalette
if (greenOrRed == "red")
palette <- redPalette
colorRampPalette(palette)(100)[10 + ceiling(90 * amount / total)]
}
# set the basic layout
layout(matrix(c(1,1,2)))
par(mar=c(2,2,2,2))
plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
title('CONFUSION MATRIX', cex.main=2)
# create the matrix
classes = colnames(cm$table)
rect(150, 430, 240, 370, col=getColor("green", res[1]))
text(195, 435, classes[1], cex=1.2)
rect(250, 430, 340, 370, col=getColor("red", res[3]))
text(295, 435, classes[2], cex=1.2)
text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
text(245, 450, 'Actual', cex=1.3, font=2)
rect(150, 305, 240, 365, col=getColor("red", res[2]))
rect(250, 305, 340, 365, col=getColor("green", res[4]))
text(140, 400, classes[1], cex=1.2, srt=90)
text(140, 335, classes[2], cex=1.2, srt=90)
# add in the cm results
text(195, 400, res[1], cex=1.6, font=2, col='white')
text(195, 335, res[2], cex=1.6, font=2, col='white')
text(295, 400, res[3], cex=1.6, font=2, col='white')
text(295, 335, res[4], cex=1.6, font=2, col='white')
# add in the specifics
plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
# add in the accuracy information
text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}
draw_confusion_matrix(cm)
bwplot(results)
AdaBTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "AdaBoost.M1",
tuneLength = 2, na.action=na.exclude, #min lenght =2
trControl = cvCtrl, metric="Accuracy")
AdaBTune
getTrainPerf(AdaBTune)
AdaBTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "AdaBoost.M1",
tuneLength = 3, na.action=na.exclude, #min lenght =2
trControl = cvCtrl, metric="Accuracy")
AdaBTune
getTrainPerf(AdaBTune)
AdaBTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "AdaBoost.M1",
tuneLength = 4, na.action=na.exclude, #min lenght =2
trControl = cvCtrl, metric="Accuracy")
AdaBTune
getTrainPerf(AdaBTune)
AdaBTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "AdaBoost.M1",
tuneLength = 5, na.action=na.exclude, #min lenght =2
trControl = cvCtrl, metric="Accuracy")
cvCtrl <- trainControl(method = "cv", number=10, search="grid", classProbs = TRUE)
NBtune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "nb",
tuneLength = 5, na.action=na.exclude,
trControl = cvCtrl, metric="Accuracy")
NBtune
getTrainPerf(NBtune)
AdaBTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "AdaBoost.M1",
tuneLength = 2, na.action=na.exclude, #min lenght =2
trControl = cvCtrl, metric="Accuracy")
AdaBTune
getTrainPerf(AdaBTune)
NBtune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "nb",
tuneLength = 10, na.action=na.exclude,
trControl = cvCtrl, metric="Accuracy")
NBtune
getTrainPerf(NBtune)
AdaBTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "AdaBoost.M1",
tuneLength = 4, na.action=na.exclude, #min lenght =2
trControl = cvCtrl, metric="Accuracy")
AdaBTune
cvCtrl <- trainControl(method = "cv", number=10, search="grid", classProbs = TRUE)
kknnT=train(ris~S1+S2+X3+S1*anno+S2*anno+anno, data = train, method = "kknn",
trControl = cvCtrl, tuneLength=5, na.action=na.exclude)
kknnT
getTrainPerf(kknnT)
kknnT=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "kknn",
trControl = cvCtrl, tuneLength=5, na.action=na.exclude)
pls=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "pls",
trControl = cvCtrl, tuneLength=10, na.action=na.exclude)
pls
getTrainPerf(pls)
kknnT
getTrainPerf(kknnT)
svmT=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "svmLinear3",
trControl = cvCtrl, tuneLength=5, na.action=na.exclude)
svmT
getTrainPerf(svmT)
kknnT=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "kknn",
trControl = cvCtrl, tuneLength=10, na.action=na.exclude)
kknnT
getTrainPerf(kknnT)
### Risultati ----
results <- resamples(list(Lasso=glm_lasso, Tree=rpartTuneCvA, RandomForest=rfTune,
NeuralNet=NNTune, XGBoosting=XGBTune, NaivaBayes=NBtune, Ada=AdaBTune,
deep=Deep, kknn=kknnT, svm=svmT));results
bwplot(results)
Deep=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "deepboost",
trControl = cvCtrl, tuneLength=5, na.action=na.exclude)
save.image("~/GitHub/DataMining/calcioWorkspaceTotale.RData")
Deep=train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "deepboost",
trControl = cvCtrl, tuneLength=3, na.action=na.exclude) #con 5 dà problemi
Deep
getTrainPerf(Deep)
### Risultati ----
results <- resamples(list(Lasso=glm_lasso, Tree=rpartTuneCvA, RandomForest=rfTune,
NeuralNet=NNTune, XGBoosting=XGBTune, NaivaBayes=NBtune, Ada=AdaBTune,
deep=Deep, kknn=kknnT, svm=svmT));results
bwplot(results)
bwplot(results)
NNTune <- train(ris~S1+S2+anno+poly(valore_rosa.x,3)+log(valore_rosa.y+1), data = train, method = "nnet",
tuneLength = 5, tuneGrid=Grid0,
trControl = cvCtrl, preProcess="range", na.action=na.exclude) # Non andare oltre il 6 di tuneLength
NNTune
getTrainPerf(NNTune)
plot(varImp(object=NNTune),main="train tuned - Variable Importance")
save.image("~/GitHub/DataMining/calcioWorkspaceTotale.RData")
